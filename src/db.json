[{"model": "contenttypes.contenttype", "pk": 1, "fields": {"app_label": "admin", "model": "logentry"}}, {"model": "contenttypes.contenttype", "pk": 2, "fields": {"app_label": "auth", "model": "permission"}}, {"model": "contenttypes.contenttype", "pk": 3, "fields": {"app_label": "auth", "model": "group"}}, {"model": "contenttypes.contenttype", "pk": 4, "fields": {"app_label": "auth", "model": "user"}}, {"model": "contenttypes.contenttype", "pk": 5, "fields": {"app_label": "contenttypes", "model": "contenttype"}}, {"model": "contenttypes.contenttype", "pk": 6, "fields": {"app_label": "sessions", "model": "session"}}, {"model": "contenttypes.contenttype", "pk": 7, "fields": {"app_label": "sites", "model": "site"}}, {"model": "contenttypes.contenttype", "pk": 8, "fields": {"app_label": "users", "model": "affiliation"}}, {"model": "contenttypes.contenttype", "pk": 9, "fields": {"app_label": "users", "model": "event"}}, {"model": "contenttypes.contenttype", "pk": 10, "fields": {"app_label": "users", "model": "profile"}}, {"model": "contenttypes.contenttype", "pk": 11, "fields": {"app_label": "users", "model": "proposal"}}, {"model": "contenttypes.contenttype", "pk": 12, "fields": {"app_label": "users", "model": "role"}}, {"model": "contenttypes.contenttype", "pk": 13, "fields": {"app_label": "users", "model": "profile_event"}}, {"model": "contenttypes.contenttype", "pk": 14, "fields": {"app_label": "users", "model": "contact"}}, {"model": "contenttypes.contenttype", "pk": 15, "fields": {"app_label": "users", "model": "partner"}}, {"model": "contenttypes.contenttype", "pk": 16, "fields": {"app_label": "users", "model": "event_partner"}}, {"model": "contenttypes.contenttype", "pk": 17, "fields": {"app_label": "users", "model": "news"}}, {"model": "contenttypes.contenttype", "pk": 18, "fields": {"app_label": "users", "model": "special_issue"}}, {"model": "contenttypes.contenttype", "pk": 19, "fields": {"app_label": "users", "model": "workshop"}}, {"model": "contenttypes.contenttype", "pk": 20, "fields": {"app_label": "users", "model": "gallery_image"}}, {"model": "contenttypes.contenttype", "pk": 21, "fields": {"app_label": "users", "model": "challenge"}}, {"model": "contenttypes.contenttype", "pk": 22, "fields": {"app_label": "users", "model": "dataset"}}, {"model": "contenttypes.contenttype", "pk": 23, "fields": {"app_label": "users", "model": "schedule_event"}}, {"model": "contenttypes.contenttype", "pk": 24, "fields": {"app_label": "users", "model": "event_relation"}}, {"model": "contenttypes.contenttype", "pk": 25, "fields": {"app_label": "users", "model": "track"}}, {"model": "contenttypes.contenttype", "pk": 26, "fields": {"app_label": "users", "model": "result"}}, {"model": "contenttypes.contenttype", "pk": 27, "fields": {"app_label": "users", "model": "score"}}, {"model": "contenttypes.contenttype", "pk": 28, "fields": {"app_label": "users", "model": "data"}}, {"model": "contenttypes.contenttype", "pk": 29, "fields": {"app_label": "users", "model": "file"}}, {"model": "contenttypes.contenttype", "pk": 30, "fields": {"app_label": "registration", "model": "registrationprofile"}}, {"model": "contenttypes.contenttype", "pk": 31, "fields": {"app_label": "watson", "model": "searchentry"}}, {"model": "contenttypes.contenttype", "pk": 32, "fields": {"app_label": "thumbnail", "model": "kvstore"}}, {"model": "sessions.session", "pk": "q1dlmu1q6dgx8id69p9qgj3jjumzw5ld", "fields": {"session_data": "NWFkOWE1MGExOTNiZWJhZjQ0MGU0ZGJmNmMzZWJmMjVmYjc3NTU4MDp7Il9hdXRoX3VzZXJfaGFzaCI6ImIzMjE5MWE3NjE4MzJlMDA0NTI3ODFiZmYwMzU0ZjUxMjJlMTliYWIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=", "expire_date": "2016-07-03T16:09:37Z"}}, {"model": "sites.site", "pk": 2, "fields": {"domain": "example.com", "name": "example.com"}}, {"model": "users.affiliation", "pk": 1, "fields": {"name": "", "country": "", "city": ""}}, {"model": "users.event", "pk": 1, "fields": {"title": "2013 Multi-modal Challenge", "description": "<p>ChaLearn organizes in 2013 a challenge and workshop on multi-modal gesture recognition from 2D and 3D video data using Kinect, in conjunction with ICMI 2013, December 9-13, Sidney, Australia.&nbsp;<strong><a href=\"http://iselab.cvc.uab.es/ChallengeMMGesture2013.pdf\" target=\"_blank\">Call for participation (pdf)</a></strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><a href=\"http://gesture.chalearn.org/2016-looking-at-people-cvpr-challenge/banner_chalearn.png?attredirects=0\"><img src=\"http://gesture.chalearn.org/_/rsrc/1453648233236/2016-looking-at-people-cvpr-challenge/banner_chalearn.png?width=600\" style=\"width:600px\" /></a><br />\r\n&nbsp;</p>\r\n\r\n<p>Kinect is revolutionizing the field of gesture recognition given the set of input data modalities it provides, including RGB image, depth image (using an infrared sensor), and audio. Gesture recognition is genuinely important in many multi-modal interaction and computer vision applications, including image/video indexing, video surveillance, computer interfaces, and gaming. It also provides excellent benchmarks for algorithms. The recognition of continuous, natural signing is very challenging due to the multimodal nature of the visual cues (e.g., movements of fingers and lips, facial expressions, body pose), as well as technical limitations such as spatial and temporal resolution and unreliable depth cues.&nbsp;</p>\r\n\r\n<p>The Multi-modal Challenge workshop will be devoted to the presentation of most recent and challenging techniques from multi-modal gesture recognition. The committee encourages paper submissions in the following topics (but not limited to):</p>\r\n\r\n<p>-Multi-modal descriptors for gesture recognition</p>\r\n\r\n<p>-Fusion strategies for gesture recognition</p>\r\n\r\n<p>-Multi-modal learning for gesture recognition</p>\r\n\r\n<p>-Data sets and evaluation protocols for multi-modal gesture recognition</p>\r\n\r\n<p>-Applications of multi-modal gesture recognition</p>\r\n\r\n<p>The results of the challenge will be discussed at the workshop. It features a quantitative evaluation of automatic gesture recognition from a multi-modal dataset recorded with Kinect (providing RGB images of face and body, depth images of face and body, skeleton information, joint orientation and audio sources), including 13,858 Italian gestures from several users. The emphasis of this edition of the competition will be on multi-modal automatic learning of a vocabulary of 20 types of Italian gestures performed by several different users while explaining a history, with the aim of performing user independent continuous gesture recognition combined with audio information.&nbsp;</p>\r\n\r\n<p>Best workshop papers and top three ranked participants of the quantitative evaluation will be invited to present their work at ICMI 2013 and their papers will be published in the ACM proceedings. Additionally, there will be travel grants (based on availability) and the possibility to be invited to present extended versions of their works to a special issue in a high impact factor journal. Moreover, all three top ranking participants in both, quantitative and qualitative challenges will be awarded with a ChaLearn winner certificate and an economic prize (based on availability). We will also announce a best paper and best student paper awards among the workshop contributions.&nbsp;</p>\r\n\r\n<p><a href=\"http://www.guardian.co.uk/travel/gallery/2009/jul/13/learn-italian-gestures-one\" target=\"_blank\"><img src=\"http://gesture.chalearn.org/_/rsrc/1453648233235/2016-looking-at-people-cvpr-challenge/SeiPazzo.jpg?height=320&amp;width=232\" style=\"height:320px; width:232px\" /></a>&nbsp;<a href=\"http://www.guardian.co.uk/travel/gallery/2009/jul/15/learn-italian-gestures-three\" target=\"_blank\"><img src=\"http://gesture.chalearn.org/_/rsrc/1453648233234/2016-looking-at-people-cvpr-challenge/Buono.jpg?height=320&amp;width=224\" style=\"height:320px; width:224px\" /></a></p>"}}, {"model": "users.event", "pk": 2, "fields": {"title": "Workshop 2013 challenge", "description": "<h3>The&nbsp;<a href=\"http://sunai.uoc.edu/chalearn/ICMI/\" target=\"_blank\">2013 Multi-modal Challenge workshop</a>&nbsp;will be held on Dec. 9th in the Level 3 Seminar Room of the&nbsp;<a href=\"http://icmi.acm.org/2013/index.php?id=venue\" target=\"_blank\">NICTA Australian Techonology Park Laboratory</a>, Redferm, in conjunction with&nbsp;<a href=\"http://icmi.acm.org/2013/index.php?id=challenges\">ICMI 2013</a>, will be devoted to the presentation of most recent and challenging techniques from multi-modal gesture recognition.</h3>"}}, {"model": "users.event", "pk": 3, "fields": {"title": "Special Issue 2013-2014", "description": "<p>The Journal of Machine Learning Research announces a new call-for-papers for the special topic on gesture recognition. Papers relevant to this topic may be&nbsp;<a href=\"http://jmlr.csail.mit.edu/author-info.html\">submitted to the journal</a>. Please also send email to the guest Editors with your paper number at&nbsp;<a href=\"mailto:gesture@clopinet.com\" target=\"_blank\">gesture @ clopinet.com&nbsp;</a><br />\r\n<br />\r\nThe participants of the 2013 ICMI&nbsp;<a href=\"https://www.kaggle.com/c/multi-modal-gesture-recognition\" target=\"_blank\">CHALEARN Multi-modal Gesture Challenge</a>&nbsp;are strongly encouraged to submit a paper.&nbsp;<br />\r\n<br />\r\n<strong>Scope</strong><br />\r\n<br />\r\nThe scope of this special issue includes, but is not limited to, the following topics:&nbsp;<br />\r\n<br />\r\n- Algorithms for gesture and activity recognition, in particular addressing:&nbsp;<br />\r\n&nbsp;</p>\r\n\r\n<p>o Learning from unlabeled or partially labeled data&nbsp;<br />\r\n<br />\r\no Learning from few examples per class, and transfer learning&nbsp;<br />\r\n<br />\r\no Continuous gesture recognition and segmentation&nbsp;<br />\r\n<br />\r\no Deep learning architectures, including convolutional neural networks&nbsp;<br />\r\n<br />\r\no Gesture recognition in challenging scenes, including cluttered/moving backgrounds or cameras&nbsp;<br />\r\n<br />\r\no Large scale gesture recognition&nbsp;<br />\r\n<br />\r\no Multi-modal features for gesture recognition, including non-conventional input sources, such as inertial, depth or thermal data&nbsp;<br />\r\n<br />\r\no Integrating information from multiple channels (e.g., position/motion of multiple body parts, hand shape, facial expressions)&nbsp;</p>\r\n\r\n<p><br />\r\n- Applications pertinent to the topic, such as:&nbsp;<br />\r\n&nbsp;</p>\r\n\r\n<p>o Video surveillance&nbsp;<br />\r\n<br />\r\no Image or video indexing and retrieval&nbsp;<br />\r\n<br />\r\no Recognition of sign languages for the deaf&nbsp;<br />\r\n<br />\r\no Emotion recognition and affective computing&nbsp;<br />\r\n<br />\r\no Computer interfaces&nbsp;<br />\r\n<br />\r\no Virtual reality&nbsp;<br />\r\n<br />\r\no Robotics&nbsp;<br />\r\n<br />\r\no Ambient intelligence&nbsp;<br />\r\n<br />\r\no Games&nbsp;</p>\r\n\r\n<p><br />\r\n- Datasets and benchmarks for gesture recognition&nbsp;<br />\r\n<br />\r\nThe papers of the special topic of JMLR will also be reprinted as a book in the&nbsp;<a href=\"http://www.mtome.com/Publications/CiML/ciml.html\">CiML series of Microtome</a>.<br />\r\n<br />\r\n<strong>Guest Editors&nbsp;</strong></p>\r\n\r\n<ul>\r\n\t<li>Dr. Isabelle Guyon, ChaLearn, Berkeley, California.</li>\r\n</ul>\r\n\r\n<ul>\r\n\t<li>Dr. Vassilis Athitsos, Texas University</li>\r\n</ul>\r\n\r\n<ul>\r\n\t<li>Dr. Sergio Escalera, University of Barcelona &amp; Computer Vision Center</li>\r\n</ul>\r\n\r\n<p>For further instructions about the submission procedure please read the JMLR policies&nbsp;<a href=\"http://jmlr.org/\">http://jmlr.org/</a>&nbsp;or send an email to the special topic guest editors to&nbsp;<a href=\"mailto:gesture@clopinet.com\">gesture @ clopinet.com</a><br />\r\n<br />\r\n<strong>Recommendations to competitors invited to write a JMLR paper:&nbsp;</strong><br />\r\n<br />\r\nThe papers wil be judged according to to following criteria:<br />\r\n&nbsp;</p>\r\n\r\n<p>(1) Performance in the challenge,&nbsp;<br />\r\n(2)Novelty/Originality,&nbsp;<br />\r\n(3) Sanity (correct proofs, good experiments),&nbsp;<br />\r\n(4) Insight, and&nbsp;<br />\r\n(5) Clarity of presentation.&nbsp;</p>\r\n\r\n<p><br />\r\nPapers merely describing the steps taken to produce a challenge entry will not be judged favorably. Please include in your submissions:<br />\r\n&nbsp;</p>\r\n\r\n<p>- The choices and advantages of the methods employed should be supported by a literature overview and qualitative and quantitative comparisons with other methods on the data of the challenge and possibly other data.&nbsp;<br />\r\n<br />\r\n- The various building blocks of the presented methods should be analyzed separately and key novel elements contributing to boosting performance significantly should be singled out.&nbsp;<br />\r\n<br />\r\n- The authors are also encouraged to motivate new approaches in a principled way and draw insights that go beyond the framework of the challenge.</p>\r\n\r\n<p><br />\r\nJMLR is a very selective publication and your paper will undergo a regular journal review. Your chances of acceptance will be increased if you:&nbsp;<br />\r\n&nbsp;</p>\r\n\r\n<p>- clearly motivate your approach from a practical and theoretical standpoint&nbsp;<br />\r\n<br />\r\n- present a consistent set of experiments (using the development data) showing a significant advantage over other methods&nbsp;<br />\r\n<br />\r\n- cite your final evaluation results in the challenge&nbsp;<br />\r\n<br />\r\n- make sure that your paper is well organized, well written, with good references, figures, and tables</p>\r\n\r\n<p><br />\r\nWe recommend not to exceed 20 pages.&nbsp;</p>"}}, {"model": "users.role", "pk": 1, "fields": {"name": "Co-chair"}}, {"model": "users.role", "pk": 2, "fields": {"name": "co-organizer"}}, {"model": "users.role", "pk": 3, "fields": {"name": "speaker"}}, {"model": "users.profile_event", "pk": 1, "fields": {"profile": 1, "event": 1, "role": 1}}, {"model": "users.profile_event", "pk": 2, "fields": {"profile": 1, "event": 2, "role": 3}}, {"model": "users.contact", "pk": 1, "fields": {"first_name": "Sergio", "last_name": "Escalera", "bio": "Sergio Escalera obtained the P.h.D. degree on Multi-class visual categorization systems at Computer Vision Center, UAB. He obtained the 2008 best Thesis award on Computer Science at Universitat Aut\u00f2noma de Barcelona. He leads the Human Pose Recovery and Behavior Analysis Group at UB, CVC, and the Barcelona Graduate School of Mathematics. He is an associate professor at the Department of Applied Mathematics and Analysis, Universitat de Barcelona. He is an adjunt professor at Universitat Oberta de Catalunya, Aalborg University, and Dalhousie University. He has been visiting professor at TU Delft and Aalborg Universities. He is a member of the Visual and Computational Learning consolidated research group of Catalonia.", "avatar": "/static/images/default.jpg", "email": "sergio.escalera@gmail.com"}}, {"model": "users.partner", "pk": 1, "fields": {"name": "Universitat de Barcelona", "url": "http://www.ub.edu/", "banner": "partner/Universitat de Barcelona/ub_ZuIw5HQ.png", "contact": 1}}, {"model": "users.event_partner", "pk": 2, "fields": {"partner": 1, "event": 1, "role": 2}}, {"model": "users.news", "pk": 1, "fields": {"title": "Initial sample data", "description": "<p><strong><a href=\"http://gesture.chalearn.org/mmdata\" target=\"_blank\">Initial sample data</a>&nbsp;for the 2013 Multi-modal Gesture Recognition Challenge released.</strong></p>", "upload_date": "2016-06-19T16:53:10Z", "event": 1}}, {"model": "users.news", "pk": 2, "fields": {"title": "Results of the Challenge", "description": "<p><strong>The results of the Challenge can be found&nbsp;<a href=\"http://iselab.cvc.uab.es/CHALEARN-MMGesture-ChallengeResults2013.pdf\" target=\"_blank\">here</a>&nbsp;and the evaluation procedure&nbsp;<a href=\"http://iselab.cvc.uab.es/ChalearnMMGR2013_ValidationProcess.pdf\" target=\"_blank\">here</a>.&nbsp;</strong></p>", "upload_date": "2016-06-19T16:54:56Z", "event": 1}}, {"model": "users.special_issue", "pk": 3, "fields": {}}, {"model": "users.workshop", "pk": 2, "fields": {}}, {"model": "users.challenge", "pk": 1, "fields": {}}, {"model": "users.dataset", "pk": 1, "fields": {"title": "Chalearn Multimodal Gesture Recognition", "description": "<p>ChaLearn organizes in 2013 a challenge and workshop on multi-modal gesture recognition from 2D and 3D video data using Kinect, in conjunction with ICMI 2013, December 9-13, Sidney, Australia. Kinect is revolutionizing the field of gesture recognition given the set of input data modalities it provides, including RGB image, depth image (using an infrared sensor), and audio. Gesture recognition is genuinely important in many multi-modal interaction and computer vision applications, including image/video indexing, video surveillance, computer interfaces, and gaming. It also provides excellent benchmarks for algorithms. The recognition of continuous, natural signing is very challenging due to the multimodal nature of the visual cues (e.g., movements of fingers and lips, facial expressions, body pose), as well as technical limitations such as spatial and temporal resolution and unreliable depth cues.</p>\r\n\r\n<h3>Reference</h3>\r\n\r\n<p>The data generated for the challenge is made available for research purposes. In case you use this data on your work, please add the following reference. You can check a detailed description of the data and results&nbsp;<a href=\"http://iselab.cvc.uab.es/Draft_Chalearn_MMGR2013_Data_Results.pdf\">here</a>.</p>\r\n\r\n<p>S. Escalera, J. Gonz&agrave;lez, X. Bar&oacute;, M. Reyes, O. Lopes, I. Guyon, V. Athistos, H.J. Escalante, &quot;Multi-modal Gesture Recognition Challenge 2013: Dataset and Results&quot;, ICMI 2013.</p>"}}, {"model": "users.schedule_event", "pk": 1, "fields": {"title": "Beginning of the competition", "description": "<p>Beginning of the quantitative competition, release of initial sample data.</p>", "date": "2013-04-30T00:00:00Z", "schedule_event_parent": null, "event_schedule": 1, "event_program": null, "dataset_schedule": null}}, {"model": "users.schedule_event", "pk": 2, "fields": {"title": "Release of development data", "description": "<p>&nbsp;Full release of development data. Validation data will be released on June 3.</p>", "date": "2013-05-25T00:00:00Z", "schedule_event_parent": null, "event_schedule": 1, "event_program": null, "dataset_schedule": null}}, {"model": "users.schedule_event", "pk": 3, "fields": {"title": "Release of final evaluation data", "description": "<p>Release of final evaluation data&nbsp;and validation ground truth labels.&nbsp;</p>", "date": "2013-08-01T00:00:00Z", "schedule_event_parent": null, "event_schedule": 1, "event_program": null, "dataset_schedule": null}}, {"model": "users.schedule_event", "pk": 4, "fields": {"title": "Deadline for submitting the fact sheets", "description": "<p>Deadline for submitting the fact sheets summarizing the proposed methods.</p>", "date": "2013-08-25T00:00:00Z", "schedule_event_parent": null, "event_schedule": 1, "event_program": null, "dataset_schedule": null}}, {"model": "users.schedule_event", "pk": 5, "fields": {"title": "Submission deadline", "description": "<p>Workshop paper submission deadline.</p>", "date": "2013-09-15T00:00:00Z", "schedule_event_parent": null, "event_schedule": 2, "event_program": null, "dataset_schedule": null}}, {"model": "users.schedule_event", "pk": 6, "fields": {"title": "Workshop paper acceptance", "description": "<p>Notification of workshop paper acceptance.</p>", "date": "2013-09-25T00:00:00Z", "schedule_event_parent": null, "event_schedule": 2, "event_program": null, "dataset_schedule": null}}, {"model": "users.schedule_event", "pk": 7, "fields": {"title": "Camera ready", "description": "<p>Camera ready of workshop papers.&nbsp;</p>", "date": "2013-10-02T00:00:00Z", "schedule_event_parent": null, "event_schedule": 2, "event_program": null, "dataset_schedule": null}}, {"model": "users.schedule_event", "pk": 8, "fields": {"title": "Opening", "description": "<p>Opening: Presentation of the workshop<br />\r\n&nbsp;</p>", "date": "2013-12-09T09:00:00Z", "schedule_event_parent": null, "event_schedule": null, "event_program": 2, "dataset_schedule": null}}, {"model": "users.schedule_event", "pk": 9, "fields": {"title": "Invited speaker I", "description": "<p><strong>Invited speaker I: Leonid Sigal, Disney Research</strong><br />\r\n&nbsp;</p>", "date": "2013-12-09T09:15:00Z", "schedule_event_parent": null, "event_schedule": null, "event_program": 2, "dataset_schedule": null}}, {"model": "users.event_relation", "pk": 1, "fields": {"challenge_relation": null, "issue_relation": null, "workshop_relation": 2, "dataset_relation": null, "event_associated": 1, "dataset_associated": null, "description": "<p>workshop of the competition.</p>"}}, {"model": "users.event_relation", "pk": 2, "fields": {"challenge_relation": null, "issue_relation": 3, "workshop_relation": null, "dataset_relation": null, "event_associated": 1, "dataset_associated": null, "description": "<p>Special issue of the competition</p>"}}, {"model": "users.track", "pk": 1, "fields": {"title": "Track 1: Human Pose Recovery", "description": "<p><strong><u>Track 1: Human Pose Recovery</u>: More than 8,000 frames of continuous RGB sequences are recorded and labeled with the objective of performing human pose recovery by means of&nbsp;recognizing more than 120,000 human limbs&nbsp;of different people. Examples of labeled frames are shown in Fig. 1.</strong></p>\r\n\r\n<p><strong><a href=\"http://gesture.chalearn.org/2016-looking-at-people-cvpr-challenge/dataset1.png?attredirects=0\"><img src=\"http://gesture.chalearn.org/_/rsrc/1453648233236/2016-looking-at-people-cvpr-challenge/dataset1.png?width=600\" style=\"width:600px\" /></a></strong></p>\r\n\r\n<p><strong><strong>Figure 1: Samples of the&nbsp;</strong>RGB Human Pose Recovery and Action/Interaction tracks.</strong></p>", "metrics": "<p>metrics</p>", "baseline": "<p>baseline</p>", "challenge": 1, "dataset": 1}}, {"model": "users.result", "pk": 1, "fields": {"user": "frkngrpnr", "challenge": 1}}, {"model": "users.result", "pk": 2, "fields": {"user": "stmater", "challenge": 1}}, {"model": "users.result", "pk": 3, "fields": {"user": "Arulkumar", "challenge": 1}}, {"model": "users.result", "pk": 4, "fields": {"user": "Arulkumar", "challenge": 1}}, {"model": "users.result", "pk": 5, "fields": {"user": "love", "challenge": 1}}, {"model": "users.result", "pk": 6, "fields": {"user": "flx", "challenge": 1}}, {"model": "users.result", "pk": 7, "fields": {"user": "flx", "challenge": 1}}, {"model": "users.result", "pk": 8, "fields": {"user": "andrewcby", "challenge": 1}}, {"model": "users.result", "pk": 9, "fields": {"user": "flx", "challenge": 1}}, {"model": "users.result", "pk": 10, "fields": {"user": "flx", "challenge": 1}}, {"model": "users.result", "pk": 11, "fields": {"user": "vponcel", "challenge": 1}}, {"model": "users.result", "pk": 12, "fields": {"user": "xbaro", "challenge": 1}}, {"model": "users.score", "pk": 1, "fields": {"name": "Accuracy", "score": 0.909625, "result": 1}}, {"model": "users.score", "pk": 2, "fields": {"name": "Extraversion", "score": 0.914528, "result": 1}}, {"model": "users.score", "pk": 3, "fields": {"name": "Agreeableness", "score": 0.910614, "result": 1}}, {"model": "users.score", "pk": 4, "fields": {"name": "Conscientiousness", "score": 0.906202, "result": 1}}, {"model": "users.score", "pk": 5, "fields": {"name": "Neuroticism", "score": 0.907215, "result": 1}}, {"model": "users.score", "pk": 6, "fields": {"name": "Openness", "score": 0.909569, "result": 1}}, {"model": "users.score", "pk": 7, "fields": {"name": "Accuracy", "score": 0.892134, "result": 2}}, {"model": "users.score", "pk": 8, "fields": {"name": "Extraversion", "score": 0.892285, "result": 2}}, {"model": "users.score", "pk": 9, "fields": {"name": "Agreeableness", "score": 0.902665, "result": 2}}, {"model": "users.score", "pk": 10, "fields": {"name": "Conscientiousness", "score": 0.884485, "result": 2}}, {"model": "users.score", "pk": 11, "fields": {"name": "Neuroticism", "score": 0.887215, "result": 2}}, {"model": "users.score", "pk": 12, "fields": {"name": "Openness", "score": 0.894021, "result": 2}}, {"model": "users.score", "pk": 13, "fields": {"name": "Accuracy", "score": 0.883436, "result": 3}}, {"model": "users.score", "pk": 14, "fields": {"name": "Extraversion", "score": 0.880569, "result": 3}}, {"model": "users.score", "pk": 15, "fields": {"name": "Agreeableness", "score": 0.899022, "result": 3}}, {"model": "users.score", "pk": 16, "fields": {"name": "Conscientiousness", "score": 0.873909, "result": 3}}, {"model": "users.score", "pk": 17, "fields": {"name": "Neuroticism", "score": 0.879019, "result": 3}}, {"model": "users.score", "pk": 18, "fields": {"name": "Openness", "score": 0.88466, "result": 3}}, {"model": "users.score", "pk": 19, "fields": {"name": "Accuracy", "score": 0.883415, "result": 4}}, {"model": "users.score", "pk": 20, "fields": {"name": "Extraversion", "score": 0.88057, "result": 4}}, {"model": "users.score", "pk": 21, "fields": {"name": "Agreeableness", "score": 0.898956, "result": 4}}, {"model": "users.score", "pk": 22, "fields": {"name": "Conscientiousness", "score": 0.873922, "result": 4}}, {"model": "users.score", "pk": 23, "fields": {"name": "Neuroticism", "score": 0.878994, "result": 4}}, {"model": "users.score", "pk": 24, "fields": {"name": "Openness", "score": 0.884631, "result": 4}}, {"model": "users.score", "pk": 25, "fields": {"name": "Accuracy", "score": 0.88159, "result": 5}}, {"model": "users.score", "pk": 26, "fields": {"name": "Extraversion", "score": 0.877942, "result": 5}}, {"model": "users.score", "pk": 27, "fields": {"name": "Agreeableness", "score": 0.897715, "result": 5}}, {"model": "users.score", "pk": 28, "fields": {"name": "Conscientiousness", "score": 0.872713, "result": 5}}, {"model": "users.score", "pk": 29, "fields": {"name": "Neuroticism", "score": 0.876856, "result": 5}}, {"model": "users.score", "pk": 30, "fields": {"name": "Openness", "score": 0.882725, "result": 5}}, {"model": "users.score", "pk": 31, "fields": {"name": "Accuracy", "score": 0.881128, "result": 6}}, {"model": "users.score", "pk": 32, "fields": {"name": "Extraversion", "score": 0.877971, "result": 6}}, {"model": "users.score", "pk": 33, "fields": {"name": "Agreeableness", "score": 0.896966, "result": 6}}, {"model": "users.score", "pk": 34, "fields": {"name": "Conscientiousness", "score": 0.872146, "result": 6}}, {"model": "users.score", "pk": 35, "fields": {"name": "Neuroticism", "score": 0.875935, "result": 6}}, {"model": "users.score", "pk": 36, "fields": {"name": "Openness", "score": 0.88262, "result": 6}}, {"model": "users.score", "pk": 37, "fields": {"name": "Accuracy", "score": 0.880058, "result": 7}}, {"model": "users.score", "pk": 38, "fields": {"name": "Extraversion", "score": 0.875604, "result": 7}}, {"model": "users.score", "pk": 39, "fields": {"name": "Agreeableness", "score": 0.897106, "result": 7}}, {"model": "users.score", "pk": 40, "fields": {"name": "Conscientiousness", "score": 0.871501, "result": 7}}, {"model": "users.score", "pk": 41, "fields": {"name": "Neuroticism", "score": 0.875625, "result": 7}}, {"model": "users.score", "pk": 42, "fields": {"name": "Openness", "score": 0.880453, "result": 7}}, {"model": "users.score", "pk": 43, "fields": {"name": "Accuracy", "score": 0.874893, "result": 8}}, {"model": "users.score", "pk": 44, "fields": {"name": "Extraversion", "score": 0.871143, "result": 8}}, {"model": "users.score", "pk": 45, "fields": {"name": "Agreeableness", "score": 0.893293, "result": 8}}, {"model": "users.score", "pk": 46, "fields": {"name": "Conscientiousness", "score": 0.86231, "result": 8}}, {"model": "users.score", "pk": 47, "fields": {"name": "Neuroticism", "score": 0.871697, "result": 8}}, {"model": "users.score", "pk": 48, "fields": {"name": "Openness", "score": 0.876024, "result": 8}}, {"model": "users.score", "pk": 49, "fields": {"name": "Accuracy", "score": 0.872707, "result": 9}}, {"model": "users.score", "pk": 50, "fields": {"name": "Extraversion", "score": 0.865557, "result": 9}}, {"model": "users.score", "pk": 51, "fields": {"name": "Agreeableness", "score": 0.891533, "result": 9}}, {"model": "users.score", "pk": 52, "fields": {"name": "Conscientiousness", "score": 0.863694, "result": 9}}, {"model": "users.score", "pk": 53, "fields": {"name": "Neuroticism", "score": 0.868377, "result": 9}}, {"model": "users.score", "pk": 54, "fields": {"name": "Openness", "score": 0.874372, "result": 9}}, {"model": "users.score", "pk": 55, "fields": {"name": "Accuracy", "score": 0.869449, "result": 10}}, {"model": "users.score", "pk": 56, "fields": {"name": "Extraversion", "score": 0.862344, "result": 10}}, {"model": "users.score", "pk": 57, "fields": {"name": "Agreeableness", "score": 0.888508, "result": 10}}, {"model": "users.score", "pk": 58, "fields": {"name": "Conscientiousness", "score": 0.86071, "result": 10}}, {"model": "users.score", "pk": 59, "fields": {"name": "Neuroticism", "score": 0.864801, "result": 10}}, {"model": "users.score", "pk": 60, "fields": {"name": "Openness", "score": 0.870882, "result": 10}}, {"model": "users.score", "pk": 61, "fields": {"name": "Accuracy", "score": 0.728085, "result": 11}}, {"model": "users.score", "pk": 62, "fields": {"name": "Extraversion", "score": 0.729942, "result": 11}}, {"model": "users.score", "pk": 63, "fields": {"name": "Agreeableness", "score": 0.736387, "result": 11}}, {"model": "users.score", "pk": 64, "fields": {"name": "Conscientiousness", "score": 0.723633, "result": 11}}, {"model": "users.score", "pk": 65, "fields": {"name": "Neuroticism", "score": 0.730032, "result": 11}}, {"model": "users.score", "pk": 66, "fields": {"name": "Openness", "score": 0.72043, "result": 11}}, {"model": "users.score", "pk": 67, "fields": {"name": "Accuracy", "score": 0.726597, "result": 12}}, {"model": "users.score", "pk": 68, "fields": {"name": "Extraversion", "score": 0.729459, "result": 12}}, {"model": "users.score", "pk": 69, "fields": {"name": "Agreeableness", "score": 0.731344, "result": 12}}, {"model": "users.score", "pk": 70, "fields": {"name": "Conscientiousness", "score": 0.725974, "result": 12}}, {"model": "users.score", "pk": 71, "fields": {"name": "Neuroticism", "score": 0.724757, "result": 12}}, {"model": "users.score", "pk": 72, "fields": {"name": "Openness", "score": 0.721453, "result": 12}}, {"model": "users.data", "pk": 1, "fields": {"title": "Training Data", "description": "<p>Training data (RGB+Depth+Audio) and labels for 393 sessions, which correspond to 7.754 italian gestures.</p>\r\n\r\n<p><img alt=\"Data sources example\" src=\"http://sunai.uoc.edu/chalearn/DataSources.jpg\" style=\"width:100%\" /></p>\r\n\r\n<p>In order to make easy the download, the data is divided into 4 files. Download all files and expand them in the same directory.</p>", "software": "<h3>DataViewer</h3>\r\n\r\n<p>Basic Matlab GUI to visualize the data (RGB, Depth and Audio) and export it in order to be used</p>\r\n\r\n<p><a href=\"http://sunai.uoc.edu/chalearn/sources/MatlabViewer.zip\">Download [25/05/2013]</a></p>\r\n\r\n<h3>Utils</h3>\r\n\r\n<p>Basic Matlab scripts for different purposes:</p>\r\n\r\n<ul>\r\n\t<li><strong>calcError.m:</strong>Get the Levenshtein distance between a provided groundtruth file and a predictions file.</li>\r\n\t<li><strong>getGestureID.m:</strong>Get the gesture ID from the gesture description.</li>\r\n\t<li><strong>getSampleData.m:</strong>If you do not want to export all the data, this script allows to access all the data from a zipped sample file. If you do not use all the modalities, you can comment out some information to speed-up the data generation.</li>\r\n</ul>\r\n\r\n<p><a href=\"http://sunai.uoc.edu/chalearn/sources/Utils.zip\">Download [26/05/2013]</a></p>", "metric": "<p>metrics</p>", "dataset": 1}}, {"model": "users.file", "pk": 1, "fields": {"name": "Training data file", "file": "datasets/Chalearn Multimodal Gesture Recognition/Training Data/trainingtest.zip", "url": null, "data": 1}}, {"model": "watson.searchentry", "pk": 1, "fields": {"engine_slug": "default", "content_type": 21, "object_id": "1", "object_id_int": 1, "title": "2013 Multi-modal Challenge", "description": "", "content": "2013 Multi-modal Challenge ChaLearn organizes in 2013 a challenge and workshop on multi-modal gesture recognition from 2D and 3D video data using Kinect, in conjunction with ICMI 2013, December 9-13, Sidney, Australia.&nbsp;Call for participation (pdf)\r\n\r\n&nbsp;\r\n\r\n\r\n&nbsp;\r\n\r\nKinect is revolutionizing the field of gesture recognition given the set of input data modalities it provides, including RGB image, depth image (using an infrared sensor), and audio. Gesture recognition is genuinely important in many multi-modal interaction and computer vision applications, including image/video indexing, video surveillance, computer interfaces, and gaming. It also provides excellent benchmarks for algorithms. The recognition of continuous, natural signing is very challenging due to the multimodal nature of the visual cues (e.g., movements of fingers and lips, facial expressions, body pose), as well as technical limitations such as spatial and temporal resolution and unreliable depth cues.&nbsp;\r\n\r\nThe Multi-modal Challenge workshop will be devoted to the presentation of most recent and challenging techniques from multi-modal gesture recognition. The committee encourages paper submissions in the following topics (but not limited to):\r\n\r\n-Multi-modal descriptors for gesture recognition\r\n\r\n-Fusion strategies for gesture recognition\r\n\r\n-Multi-modal learning for gesture recognition\r\n\r\n-Data sets and evaluation protocols for multi-modal gesture recognition\r\n\r\n-Applications of multi-modal gesture recognition\r\n\r\nThe results of the challenge will be discussed at the workshop. It features a quantitative evaluation of automatic gesture recognition from a multi-modal dataset recorded with Kinect (providing RGB images of face and body, depth images of face and body, skeleton information, joint orientation and audio sources), including 13,858 Italian gestures from several users. The emphasis of this edition of the competition will be on multi-modal automatic learning of a vocabulary of 20 types of Italian gestures performed by several different users while explaining a history, with the aim of performing user independent continuous gesture recognition combined with audio information.&nbsp;\r\n\r\nBest workshop papers and top three ranked participants of the quantitative evaluation will be invited to present their work at ICMI 2013 and their papers will be published in the ACM proceedings. Additionally, there will be travel grants (based on availability) and the possibility to be invited to present extended versions of their works to a special issue in a high impact factor journal. Moreover, all three top ranking participants in both, quantitative and qualitative challenges will be awarded with a ChaLearn winner certificate and an economic prize (based on availability). We will also announce a best paper and best student paper awards among the workshop contributions.&nbsp;\r\n\r\n&nbsp;", "url": "/challenge/1/description/", "meta_encoded": "{}"}}, {"model": "watson.searchentry", "pk": 2, "fields": {"engine_slug": "default", "content_type": 19, "object_id": "2", "object_id_int": 2, "title": "Workshop 2013 challenge", "description": "", "content": "Workshop 2013 challenge The&nbsp;2013 Multi-modal Challenge workshop&nbsp;will be held on Dec. 9th in the Level 3 Seminar Room of the&nbsp;NICTA Australian Techonology Park Laboratory, Redferm, in conjunction with&nbsp;ICMI 2013, will be devoted to the presentation of most recent and challenging techniques from multi-modal gesture recognition.", "url": "/workshop/2/description/", "meta_encoded": "{}"}}, {"model": "watson.searchentry", "pk": 3, "fields": {"engine_slug": "default", "content_type": 18, "object_id": "3", "object_id_int": 3, "title": "Special Issue 2013-2014", "description": "", "content": "Special Issue 2013-2014 The Journal of Machine Learning Research announces a new call-for-papers for the special topic on gesture recognition. Papers relevant to this topic may be&nbsp;submitted to the journal. Please also send email to the guest Editors with your paper number at&nbsp;gesture @ clopinet.com&nbsp;\r\n\r\nThe participants of the 2013 ICMI&nbsp;CHALEARN Multi-modal Gesture Challenge&nbsp;are strongly encouraged to submit a paper.&nbsp;\r\n\r\nScope\r\n\r\nThe scope of this special issue includes, but is not limited to, the following topics:&nbsp;\r\n\r\n- Algorithms for gesture and activity recognition, in particular addressing:&nbsp;\r\n&nbsp;\r\n\r\no Learning from unlabeled or partially labeled data&nbsp;\r\n\r\no Learning from few examples per class, and transfer learning&nbsp;\r\n\r\no Continuous gesture recognition and segmentation&nbsp;\r\n\r\no Deep learning architectures, including convolutional neural networks&nbsp;\r\n\r\no Gesture recognition in challenging scenes, including cluttered/moving backgrounds or cameras&nbsp;\r\n\r\no Large scale gesture recognition&nbsp;\r\n\r\no Multi-modal features for gesture recognition, including non-conventional input sources, such as inertial, depth or thermal data&nbsp;\r\n\r\no Integrating information from multiple channels (e.g., position/motion of multiple body parts, hand shape, facial expressions)&nbsp;\r\n\r\n\r\n- Applications pertinent to the topic, such as:&nbsp;\r\n&nbsp;\r\n\r\no Video surveillance&nbsp;\r\n\r\no Image or video indexing and retrieval&nbsp;\r\n\r\no Recognition of sign languages for the deaf&nbsp;\r\n\r\no Emotion recognition and affective computing&nbsp;\r\n\r\no Computer interfaces&nbsp;\r\n\r\no Virtual reality&nbsp;\r\n\r\no Robotics&nbsp;\r\n\r\no Ambient intelligence&nbsp;\r\n\r\no Games&nbsp;\r\n\r\n\r\n- Datasets and benchmarks for gesture recognition&nbsp;\r\n\r\nThe papers of the special topic of JMLR will also be reprinted as a book in the&nbsp;CiML series of Microtome.\r\n\r\nGuest Editors&nbsp;\r\n\r\n\r\n\tDr. Isabelle Guyon, ChaLearn, Berkeley, California.\r\n\r\n\r\n\r\n\tDr. Vassilis Athitsos, Texas University\r\n\r\n\r\n\r\n\tDr. Sergio Escalera, University of Barcelona &amp; Computer Vision Center\r\n\r\n\r\nFor further instructions about the submission procedure please read the JMLR policies&nbsp;http://jmlr.org/&nbsp;or send an email to the special topic guest editors to&nbsp;gesture @ clopinet.com\r\n\r\nRecommendations to competitors invited to write a JMLR paper:&nbsp;\r\n\r\nThe papers wil be judged according to to following criteria:\r\n&nbsp;\r\n\r\n(1) Performance in the challenge,&nbsp;\r\n(2)Novelty/Originality,&nbsp;\r\n(3) Sanity (correct proofs, good experiments),&nbsp;\r\n(4) Insight, and&nbsp;\r\n(5) Clarity of presentation.&nbsp;\r\n\r\n\r\nPapers merely describing the steps taken to produce a challenge entry will not be judged favorably. Please include in your submissions:\r\n&nbsp;\r\n\r\n- The choices and advantages of the methods employed should be supported by a literature overview and qualitative and quantitative comparisons with other methods on the data of the challenge and possibly other data.&nbsp;\r\n\r\n- The various building blocks of the presented methods should be analyzed separately and key novel elements contributing to boosting performance significantly should be singled out.&nbsp;\r\n\r\n- The authors are also encouraged to motivate new approaches in a principled way and draw insights that go beyond the framework of the challenge.\r\n\r\n\r\nJMLR is a very selective publication and your paper will undergo a regular journal review. Your chances of acceptance will be increased if you:&nbsp;\r\n&nbsp;\r\n\r\n- clearly motivate your approach from a practical and theoretical standpoint&nbsp;\r\n\r\n- present a consistent set of experiments (using the development data) showing a significant advantage over other methods&nbsp;\r\n\r\n- cite your final evaluation results in the challenge&nbsp;\r\n\r\n- make sure that your paper is well organized, well written, with good references, figures, and tables\r\n\r\n\r\nWe recommend not to exceed 20 pages.&nbsp;", "url": "/specialissue/3/description/", "meta_encoded": "{}"}}, {"model": "watson.searchentry", "pk": 4, "fields": {"engine_slug": "default", "content_type": 22, "object_id": "1", "object_id_int": 1, "title": "Chalearn Multimodal Gesture Recognition", "description": "", "content": "Chalearn Multimodal Gesture Recognition ChaLearn organizes in 2013 a challenge and workshop on multi-modal gesture recognition from 2D and 3D video data using Kinect, in conjunction with ICMI 2013, December 9-13, Sidney, Australia. Kinect is revolutionizing the field of gesture recognition given the set of input data modalities it provides, including RGB image, depth image (using an infrared sensor), and audio. Gesture recognition is genuinely important in many multi-modal interaction and computer vision applications, including image/video indexing, video surveillance, computer interfaces, and gaming. It also provides excellent benchmarks for algorithms. The recognition of continuous, natural signing is very challenging due to the multimodal nature of the visual cues (e.g., movements of fingers and lips, facial expressions, body pose), as well as technical limitations such as spatial and temporal resolution and unreliable depth cues.\r\n\r\nReference\r\n\r\nThe data generated for the challenge is made available for research purposes. In case you use this data on your work, please add the following reference. You can check a detailed description of the data and results&nbsp;here.\r\n\r\nS. Escalera, J. Gonz&agrave;lez, X. Bar&oacute;, M. Reyes, O. Lopes, I. Guyon, V. Athistos, H.J. Escalante, &quot;Multi-modal Gesture Recognition Challenge 2013: Dataset and Results&quot;, ICMI 2013.", "url": "/dataset/1/description/", "meta_encoded": "{}"}}, {"model": "auth.permission", "pk": 1, "fields": {"name": "Can add log entry", "content_type": 1, "codename": "add_logentry"}}, {"model": "auth.permission", "pk": 2, "fields": {"name": "Can change log entry", "content_type": 1, "codename": "change_logentry"}}, {"model": "auth.permission", "pk": 3, "fields": {"name": "Can delete log entry", "content_type": 1, "codename": "delete_logentry"}}, {"model": "auth.permission", "pk": 4, "fields": {"name": "Can add permission", "content_type": 2, "codename": "add_permission"}}, {"model": "auth.permission", "pk": 5, "fields": {"name": "Can change permission", "content_type": 2, "codename": "change_permission"}}, {"model": "auth.permission", "pk": 6, "fields": {"name": "Can delete permission", "content_type": 2, "codename": "delete_permission"}}, {"model": "auth.permission", "pk": 7, "fields": {"name": "Can add group", "content_type": 3, "codename": "add_group"}}, {"model": "auth.permission", "pk": 8, "fields": {"name": "Can change group", "content_type": 3, "codename": "change_group"}}, {"model": "auth.permission", "pk": 9, "fields": {"name": "Can delete group", "content_type": 3, "codename": "delete_group"}}, {"model": "auth.permission", "pk": 10, "fields": {"name": "Can add user", "content_type": 4, "codename": "add_user"}}, {"model": "auth.permission", "pk": 11, "fields": {"name": "Can change user", "content_type": 4, "codename": "change_user"}}, {"model": "auth.permission", "pk": 12, "fields": {"name": "Can delete user", "content_type": 4, "codename": "delete_user"}}, {"model": "auth.permission", "pk": 13, "fields": {"name": "Can add content type", "content_type": 5, "codename": "add_contenttype"}}, {"model": "auth.permission", "pk": 14, "fields": {"name": "Can change content type", "content_type": 5, "codename": "change_contenttype"}}, {"model": "auth.permission", "pk": 15, "fields": {"name": "Can delete content type", "content_type": 5, "codename": "delete_contenttype"}}, {"model": "auth.permission", "pk": 16, "fields": {"name": "Can add session", "content_type": 6, "codename": "add_session"}}, {"model": "auth.permission", "pk": 17, "fields": {"name": "Can change session", "content_type": 6, "codename": "change_session"}}, {"model": "auth.permission", "pk": 18, "fields": {"name": "Can delete session", "content_type": 6, "codename": "delete_session"}}, {"model": "auth.permission", "pk": 19, "fields": {"name": "Can add site", "content_type": 7, "codename": "add_site"}}, {"model": "auth.permission", "pk": 20, "fields": {"name": "Can change site", "content_type": 7, "codename": "change_site"}}, {"model": "auth.permission", "pk": 21, "fields": {"name": "Can delete site", "content_type": 7, "codename": "delete_site"}}, {"model": "auth.permission", "pk": 22, "fields": {"name": "Can add affiliation", "content_type": 8, "codename": "add_affiliation"}}, {"model": "auth.permission", "pk": 23, "fields": {"name": "Can change affiliation", "content_type": 8, "codename": "change_affiliation"}}, {"model": "auth.permission", "pk": 24, "fields": {"name": "Can delete affiliation", "content_type": 8, "codename": "delete_affiliation"}}, {"model": "auth.permission", "pk": 25, "fields": {"name": "Can add event", "content_type": 9, "codename": "add_event"}}, {"model": "auth.permission", "pk": 26, "fields": {"name": "Can change event", "content_type": 9, "codename": "change_event"}}, {"model": "auth.permission", "pk": 27, "fields": {"name": "Can delete event", "content_type": 9, "codename": "delete_event"}}, {"model": "auth.permission", "pk": 28, "fields": {"name": "Can add profile", "content_type": 10, "codename": "add_profile"}}, {"model": "auth.permission", "pk": 29, "fields": {"name": "Can change profile", "content_type": 10, "codename": "change_profile"}}, {"model": "auth.permission", "pk": 30, "fields": {"name": "Can delete profile", "content_type": 10, "codename": "delete_profile"}}, {"model": "auth.permission", "pk": 31, "fields": {"name": "Can add proposal", "content_type": 11, "codename": "add_proposal"}}, {"model": "auth.permission", "pk": 32, "fields": {"name": "Can change proposal", "content_type": 11, "codename": "change_proposal"}}, {"model": "auth.permission", "pk": 33, "fields": {"name": "Can delete proposal", "content_type": 11, "codename": "delete_proposal"}}, {"model": "auth.permission", "pk": 34, "fields": {"name": "Can add role", "content_type": 12, "codename": "add_role"}}, {"model": "auth.permission", "pk": 35, "fields": {"name": "Can change role", "content_type": 12, "codename": "change_role"}}, {"model": "auth.permission", "pk": 36, "fields": {"name": "Can delete role", "content_type": 12, "codename": "delete_role"}}, {"model": "auth.permission", "pk": 37, "fields": {"name": "Can add profile_ event", "content_type": 13, "codename": "add_profile_event"}}, {"model": "auth.permission", "pk": 38, "fields": {"name": "Can change profile_ event", "content_type": 13, "codename": "change_profile_event"}}, {"model": "auth.permission", "pk": 39, "fields": {"name": "Can delete profile_ event", "content_type": 13, "codename": "delete_profile_event"}}, {"model": "auth.permission", "pk": 40, "fields": {"name": "Can add contact", "content_type": 14, "codename": "add_contact"}}, {"model": "auth.permission", "pk": 41, "fields": {"name": "Can change contact", "content_type": 14, "codename": "change_contact"}}, {"model": "auth.permission", "pk": 42, "fields": {"name": "Can delete contact", "content_type": 14, "codename": "delete_contact"}}, {"model": "auth.permission", "pk": 43, "fields": {"name": "Can add partner", "content_type": 15, "codename": "add_partner"}}, {"model": "auth.permission", "pk": 44, "fields": {"name": "Can change partner", "content_type": 15, "codename": "change_partner"}}, {"model": "auth.permission", "pk": 45, "fields": {"name": "Can delete partner", "content_type": 15, "codename": "delete_partner"}}, {"model": "auth.permission", "pk": 46, "fields": {"name": "Can add event_ partner", "content_type": 16, "codename": "add_event_partner"}}, {"model": "auth.permission", "pk": 47, "fields": {"name": "Can change event_ partner", "content_type": 16, "codename": "change_event_partner"}}, {"model": "auth.permission", "pk": 48, "fields": {"name": "Can delete event_ partner", "content_type": 16, "codename": "delete_event_partner"}}, {"model": "auth.permission", "pk": 49, "fields": {"name": "Can add news", "content_type": 17, "codename": "add_news"}}, {"model": "auth.permission", "pk": 50, "fields": {"name": "Can change news", "content_type": 17, "codename": "change_news"}}, {"model": "auth.permission", "pk": 51, "fields": {"name": "Can delete news", "content_type": 17, "codename": "delete_news"}}, {"model": "auth.permission", "pk": 52, "fields": {"name": "Can add special_ issue", "content_type": 18, "codename": "add_special_issue"}}, {"model": "auth.permission", "pk": 53, "fields": {"name": "Can change special_ issue", "content_type": 18, "codename": "change_special_issue"}}, {"model": "auth.permission", "pk": 54, "fields": {"name": "Can delete special_ issue", "content_type": 18, "codename": "delete_special_issue"}}, {"model": "auth.permission", "pk": 55, "fields": {"name": "Can add workshop", "content_type": 19, "codename": "add_workshop"}}, {"model": "auth.permission", "pk": 56, "fields": {"name": "Can change workshop", "content_type": 19, "codename": "change_workshop"}}, {"model": "auth.permission", "pk": 57, "fields": {"name": "Can delete workshop", "content_type": 19, "codename": "delete_workshop"}}, {"model": "auth.permission", "pk": 58, "fields": {"name": "Can add gallery_ image", "content_type": 20, "codename": "add_gallery_image"}}, {"model": "auth.permission", "pk": 59, "fields": {"name": "Can change gallery_ image", "content_type": 20, "codename": "change_gallery_image"}}, {"model": "auth.permission", "pk": 60, "fields": {"name": "Can delete gallery_ image", "content_type": 20, "codename": "delete_gallery_image"}}, {"model": "auth.permission", "pk": 61, "fields": {"name": "Can add challenge", "content_type": 21, "codename": "add_challenge"}}, {"model": "auth.permission", "pk": 62, "fields": {"name": "Can change challenge", "content_type": 21, "codename": "change_challenge"}}, {"model": "auth.permission", "pk": 63, "fields": {"name": "Can delete challenge", "content_type": 21, "codename": "delete_challenge"}}, {"model": "auth.permission", "pk": 64, "fields": {"name": "Can add dataset", "content_type": 22, "codename": "add_dataset"}}, {"model": "auth.permission", "pk": 65, "fields": {"name": "Can change dataset", "content_type": 22, "codename": "change_dataset"}}, {"model": "auth.permission", "pk": 66, "fields": {"name": "Can delete dataset", "content_type": 22, "codename": "delete_dataset"}}, {"model": "auth.permission", "pk": 67, "fields": {"name": "Can add schedule_ event", "content_type": 23, "codename": "add_schedule_event"}}, {"model": "auth.permission", "pk": 68, "fields": {"name": "Can change schedule_ event", "content_type": 23, "codename": "change_schedule_event"}}, {"model": "auth.permission", "pk": 69, "fields": {"name": "Can delete schedule_ event", "content_type": 23, "codename": "delete_schedule_event"}}, {"model": "auth.permission", "pk": 70, "fields": {"name": "Can add event_ relation", "content_type": 24, "codename": "add_event_relation"}}, {"model": "auth.permission", "pk": 71, "fields": {"name": "Can change event_ relation", "content_type": 24, "codename": "change_event_relation"}}, {"model": "auth.permission", "pk": 72, "fields": {"name": "Can delete event_ relation", "content_type": 24, "codename": "delete_event_relation"}}, {"model": "auth.permission", "pk": 73, "fields": {"name": "Can add track", "content_type": 25, "codename": "add_track"}}, {"model": "auth.permission", "pk": 74, "fields": {"name": "Can change track", "content_type": 25, "codename": "change_track"}}, {"model": "auth.permission", "pk": 75, "fields": {"name": "Can delete track", "content_type": 25, "codename": "delete_track"}}, {"model": "auth.permission", "pk": 76, "fields": {"name": "Can add result", "content_type": 26, "codename": "add_result"}}, {"model": "auth.permission", "pk": 77, "fields": {"name": "Can change result", "content_type": 26, "codename": "change_result"}}, {"model": "auth.permission", "pk": 78, "fields": {"name": "Can delete result", "content_type": 26, "codename": "delete_result"}}, {"model": "auth.permission", "pk": 79, "fields": {"name": "Can add score", "content_type": 27, "codename": "add_score"}}, {"model": "auth.permission", "pk": 80, "fields": {"name": "Can change score", "content_type": 27, "codename": "change_score"}}, {"model": "auth.permission", "pk": 81, "fields": {"name": "Can delete score", "content_type": 27, "codename": "delete_score"}}, {"model": "auth.permission", "pk": 82, "fields": {"name": "Can add data", "content_type": 28, "codename": "add_data"}}, {"model": "auth.permission", "pk": 83, "fields": {"name": "Can change data", "content_type": 28, "codename": "change_data"}}, {"model": "auth.permission", "pk": 84, "fields": {"name": "Can delete data", "content_type": 28, "codename": "delete_data"}}, {"model": "auth.permission", "pk": 85, "fields": {"name": "Can add file", "content_type": 29, "codename": "add_file"}}, {"model": "auth.permission", "pk": 86, "fields": {"name": "Can change file", "content_type": 29, "codename": "change_file"}}, {"model": "auth.permission", "pk": 87, "fields": {"name": "Can delete file", "content_type": 29, "codename": "delete_file"}}, {"model": "auth.permission", "pk": 88, "fields": {"name": "Can add registration profile", "content_type": 30, "codename": "add_registrationprofile"}}, {"model": "auth.permission", "pk": 89, "fields": {"name": "Can change registration profile", "content_type": 30, "codename": "change_registrationprofile"}}, {"model": "auth.permission", "pk": 90, "fields": {"name": "Can delete registration profile", "content_type": 30, "codename": "delete_registrationprofile"}}, {"model": "auth.permission", "pk": 91, "fields": {"name": "Can add search entry", "content_type": 31, "codename": "add_searchentry"}}, {"model": "auth.permission", "pk": 92, "fields": {"name": "Can change search entry", "content_type": 31, "codename": "change_searchentry"}}, {"model": "auth.permission", "pk": 93, "fields": {"name": "Can delete search entry", "content_type": 31, "codename": "delete_searchentry"}}, {"model": "auth.permission", "pk": 94, "fields": {"name": "Can add kv store", "content_type": 32, "codename": "add_kvstore"}}, {"model": "auth.permission", "pk": 95, "fields": {"name": "Can change kv store", "content_type": 32, "codename": "change_kvstore"}}, {"model": "auth.permission", "pk": 96, "fields": {"name": "Can delete kv store", "content_type": 32, "codename": "delete_kvstore"}}, {"model": "auth.user", "pk": 1, "fields": {"password": "pbkdf2_sha256$24000$cNfcPxzc7HLK$C8mxJE6rwKfB/ipXSHw5rDfCD+jJIzzxcrbN3PmlGd0=", "last_login": "2016-06-19T16:09:37Z", "is_superuser": true, "username": "oriol", "first_name": "", "last_name": "", "email": "", "is_staff": true, "is_active": true, "date_joined": "2016-06-19T16:09:31Z", "groups": [], "user_permissions": []}}, {"model": "auth.user", "pk": 2, "fields": {"password": "pbkdf2_sha256$24000$3IJrIAlqdOzr$1QTB/+Dy6OkiK3taPFAhqMV52Q02ystpOsPmujYEGgQ=", "last_login": null, "is_superuser": false, "username": "sergioescalera", "first_name": "", "last_name": "", "email": "sergioescalera@gmail.com", "is_staff": false, "is_active": false, "date_joined": "2016-06-19T17:21:40Z", "groups": [], "user_permissions": []}}, {"model": "users.profile", "pk": 1, "fields": {"user": 2, "first_name": "Sergio", "last_name": "Escalera", "affiliation": 1, "bio": "", "avatar": ""}}, {"model": "registration.registrationprofile", "pk": 1, "fields": {"user": 2, "activation_key": "650f0a7ef4f27c3973d992f1801a60cc29afd459", "activated": false}}]